import os
import random
import shutil
import torch
import argparse
from torchvision import transforms as T
from torchvision.transforms import functional as TF
from PIL import Image
from pathlib import Path
from timm.models.vision_transformer import VisionTransformer
from functools import partial
from torch import nn
from huggingface_hub import snapshot_download
from tqdm import tqdm
import time

# ---------- è³‡æ–™å¢å¼·èˆ‡è¤‡è£½ ----------
def prepare_augmented_dataset(input_dir="test_images", output_dir="all_images", input_size=224):
    """
    è‹¥ input_dir åç¨±åŒ…å« 'train' -> å°å½±åƒåš resize ä¸¦åœ¨æª”å A/C æ™‚åš augmentationï¼ˆåŒ…å«é«˜æ–¯é›œè¨Šï¼‰
    è‹¥ input_dir åç¨±åŒ…å« 'val'   -> åªè¤‡è£½åŸå§‹æª”æ¡ˆåˆ° output_dirï¼ˆä¸åš resize / augmentï¼‰
    """
    os.makedirs(output_dir, exist_ok=True)

    input_path = Path(input_dir)
    image_paths = sorted(input_path.glob("*.png")) + sorted(input_path.glob("*.jpg"))

    if len(image_paths) == 0:
        raise FileNotFoundError("âŒ æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡ï¼Œè«‹ç¢ºèªè³‡æ–™å¤¾æ˜¯å¦æ­£ç¢ºä¸”åŒ…å« .png æˆ– .jpg æª”æ¡ˆï¼")

    is_train = "train" in input_dir.lower()
    is_val = "val" in input_dir.lower()

    print(f"ğŸ“ è³‡æ–™å¤¾: {input_dir}")
    if is_train:
        print("æ¨¡å¼: è¨“ç·´é›† (é€²è¡Œ resize èˆ‡è³‡æ–™å¢å¼·)")
    elif is_val:
        print("æ¨¡å¼: é©—è­‰é›† (åƒ…è¤‡è£½åŸå§‹æª”æ¡ˆï¼Œä¸åš resize / augment)")
    else:
        # é è¨­è¡Œç‚ºï¼šè‹¥è·¯å¾‘æ—¢ä¸å« train ä¹Ÿä¸å« valï¼Œç•¶ä½œ train è™•ç†ï¼ˆå¯è¦–éœ€æ±‚æ”¹ï¼‰
        print("è­¦å‘Š: input è³‡æ–™å¤¾åç¨±æœªåŒ…å« 'train' æˆ– 'val'ï¼Œé è¨­ç‚º train æ¨¡å¼ï¼ˆæœƒåšå¢å¼·ï¼‰")
        is_train = True

    base_transform = T.Resize((input_size, input_size))

    def add_gaussian_noise(img):
        """å°å½±åƒæ–°å¢é«˜æ–¯é›œè¨Šï¼Œè¼¸å…¥/è¼¸å‡ºç‚º PIL Image"""
        std = 0.05  # å¯èª¿æ•´é›œè¨Šå¼·åº¦
        tensor = TF.to_tensor(img)
        noise = torch.randn_like(tensor) * std
        noisy_tensor = torch.clamp(tensor + noise, 0, 1)
        return TF.to_pil_image(noisy_tensor)

    augmentations = [
        lambda x: TF.hflip(x),
        lambda x: TF.vflip(x),
        lambda x: TF.rotate(x, angle=random.choice([15, -15, 30, -30])),
        lambda x: TF.adjust_brightness(x, 1.2),
        lambda x: TF.adjust_contrast(x, 1.3),
        lambda x: TF.adjust_saturation(x, 1.4),
        lambda x: add_gaussian_noise(x),
    ]

    print(f"æ­£åœ¨è™•ç† {len(image_paths)} å¼µåœ–ç‰‡...")
    for p in tqdm(image_paths, desc="è³‡æ–™æº–å‚™", position=0):
        if is_val:
            # åªè¤‡è£½åŸå§‹æª”æ¡ˆï¼ˆä¸æ”¹æª”åã€ä¸ resizeï¼‰
            shutil.copy2(p, Path(output_dir) / p.name)
            continue

        # ä»¥ä¸‹ç‚º train çš„è™•ç†ï¼ˆresize ä¸¦è¦–æƒ…æ³åš augmentationï¼‰
        img = Image.open(p).convert("RGB")

        # å„²å­˜ resize å¾Œç‰ˆæœ¬
        resized = base_transform(img)
        resized.save(Path(output_dir) / p.name)

        # è‹¥æª”åé–‹é ­æ˜¯ A æˆ– Cï¼Œé¡å¤–ç”¢ç”Ÿ augmentation å½±åƒ
        if p.name[0].upper() in ["A", "C"]:
            aug_fn = random.choice(augmentations)
            aug_img = aug_fn(img)
            aug_resized = base_transform(aug_img)
            name, ext = os.path.splitext(p.name)
            aug_resized.save(Path(output_dir) / f"{name}_aug{ext}")

    print("âœ… è³‡æ–™æº–å‚™å®Œæˆï¼")
    return is_train  # å›å‚³æ˜¯å¦ç‚º train æ¨¡å¼ï¼Œä»¥ä¾¿ä¸»ç¨‹å¼æ±ºå®šæ˜¯å¦ç¹¼çºŒå¾ŒçºŒè™•ç†


# ---------- åœ–ç‰‡å‰è™•ç† ----------
def process_single_image(image_path, input_size=224,
                         dataset_mean=[0.3464, 0.2280, 0.2228],
                         dataset_std=[0.2520, 0.2128, 0.2093]):
    transform = T.Compose([
        T.Resize((input_size, input_size)),
        T.ToTensor(),
        T.Normalize(mean=dataset_mean, std=dataset_std)
    ])
    image = Image.open(image_path).convert('RGB')
    processed = transform(image)
    return processed


# ---------- è¼‰å…¥æ¨¡å‹ ----------
def load_model_from_huggingface(repo_id, model_filename="pytorch_model.bin"):
    model_path = snapshot_download(repo_id=repo_id, revision="main")
    model_weights_path = Path(model_path) / model_filename

    ckpt = torch.load(model_weights_path, map_location="cpu")
    model_weights = ckpt.get('model', ckpt)

    model = VisionTransformer(
        patch_size=16,
        embed_dim=768,
        depth=12,
        num_heads=12,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6)
    ).eval()

    loading = model.load_state_dict(model_weights, strict=False)
    return model, loading


# ---------- ä¸»ç¨‹å¼ ----------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å½±åƒç‰¹å¾µæ“·å–å·¥å…·ï¼ˆtrain/val æ¢ä»¶å¼è™•ç†ï¼‰")
    parser.add_argument("--input", type=str, default=r"C:\å°å¤§ç¢©å£«è³‡æ–™\å¯¦é©—å®¤\hysteroscopy\TrainTestDataset\val", help="è¼¸å…¥è³‡æ–™å¤¾è·¯å¾‘")
    parser.add_argument("--output", type=str, default="all_images", help="è¼¸å‡ºè³‡æ–™å¤¾è·¯å¾‘")
    parser.add_argument("--size", type=int, default=224, help="å½±åƒè¼¸å…¥å¤§å°ï¼ˆtrain æ¨¡å¼ä¸‹æœƒè¢«ç”¨ä¾† resizeï¼‰")
    args = parser.parse_args()

    start_time = time.time()

    try:
        # æº–å‚™è³‡æ–™é›†ï¼šè‹¥æ˜¯ val æ¨¡å¼ï¼Œè©²å‡½å¼åªæœƒè¤‡è£½åŸå§‹æª”æ¡ˆä¸¦å›å‚³ is_train=False
        prepare_augmented_dataset(args.input, args.output, input_size=224)


        # ä»¥ä¸‹ç‚º train æ¨¡å¼ï¼šè®€å– output è³‡æ–™å¤¾çš„å½±åƒä¸¦åšå‰è™•ç†èˆ‡ç‰¹å¾µæ“·å–
        image_folder = Path(args.output)
        image_paths = sorted(image_folder.glob("*.png")) + sorted(image_folder.glob("*.jpg"))

        if len(image_paths) == 0:
            raise FileNotFoundError("âŒ Output è³‡æ–™å¤¾æ²’æœ‰ä»»ä½•åœ–ç‰‡ï¼Œè«‹æª¢æŸ¥ prepare_augmented_dataset æ˜¯å¦æˆåŠŸã€‚")

        images_list = []
        for p in tqdm(image_paths, desc="è™•ç†åœ–ç‰‡", position=0):
            images_list.append(process_single_image(p, input_size=args.size))
        images = torch.stack(images_list)

        # è¨­å®šè£ç½® (è‡ªå‹•åµæ¸¬ GPU)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        dtype = torch.float32

        # è¼‰å…¥æ¨¡å‹
        model, loading_info = load_model_from_huggingface("egeozsoy/EndoViT")
        model = model.to(device, dtype)
        print(f"\næ¨¡å‹è¼‰å…¥å®Œæˆï¼Œä½¿ç”¨è£ç½®ï¼š{device}")

        # å‰å‘å‚³æ’­
        images = images.to(device, dtype)
        features_list = []
        for i in tqdm(range(images.shape[0]), desc="è¨ˆç®—ç‰¹å¾µ", position=0):
            with torch.no_grad():
                feat = model.forward_features(images[i:i+1])
                features_list.append(feat)
        features = torch.cat(features_list, dim=0)

        # å­˜æª”
        torch.save(features.cpu(), "features.pt")
        print("ğŸ’¾ ç‰¹å¾µå·²å­˜æˆ features.pt")

    except KeyboardInterrupt:
        print("\nğŸŸ¥ åµæ¸¬åˆ° Ctrl+Cï¼Œä¸­æ–·ç¨‹å¼...")

    end_time = time.time()
    print(f"â±ï¸ ç¸½åŸ·è¡Œæ™‚é–“: {end_time - start_time:.1f} ç§’")
