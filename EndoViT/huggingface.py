import torch
from torchvision import transforms as T
from PIL import Image
from pathlib import Path
from timm.models.vision_transformer import VisionTransformer
from functools import partial
from torch import nn
from huggingface_hub import snapshot_download
import psutil
import threading
import time
from tqdm import tqdm
import sys

# ---------- å¯¦æ™‚å–®è¡Œç›£æ§å‡½æ•¸ ----------
def monitor_resources(interval=1):
    while monitor_resources.running:
        cpu_percent = psutil.cpu_percent(interval=None)
        ram = psutil.virtual_memory()
        print(f"\033[F[è³‡æºç›£æ§] CPU: {cpu_percent:.1f}% | RAM: {ram.used / 1024**2:.1f}MB / {ram.total / 1024**2:.1f}MB")
        time.sleep(interval)
    print()  # åœæ­¢æ™‚æ›è¡Œ

monitor_resources.running = True

# ---------- åœ–ç‰‡å‰è™•ç† ----------
def process_single_image(image_path, input_size=224,
                         dataset_mean=[0.3464, 0.2280, 0.2228],
                         dataset_std=[0.2520, 0.2128, 0.2093]):
    transform = T.Compose([
        T.Resize((input_size, input_size)),
        T.ToTensor(),
        T.Normalize(mean=dataset_mean, std=dataset_std)
    ])
    image = Image.open(image_path).convert('RGB')
    processed = transform(image)
    return processed

# ---------- è¼‰å…¥æ¨¡å‹ ----------
def load_model_from_huggingface(repo_id, model_filename="pytorch_model.bin"):
    model_path = snapshot_download(repo_id=repo_id, revision="main")
    model_weights_path = Path(model_path) / model_filename

    ckpt = torch.load(model_weights_path, map_location="cpu")
    model_weights = ckpt.get('model', ckpt)

    model = VisionTransformer(
        patch_size=16,
        embed_dim=768,
        depth=12,
        num_heads=12,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6)
    ).eval()

    loading = model.load_state_dict(model_weights, strict=False)
    return model, loading

if __name__ == "__main__":
    start_time = time.time()

    # å…ˆè¼¸å‡ºç©ºè¡Œçµ¦ç›£æ§ä½¿ç”¨
    print()  # ç©ºè¡Œè®“è³‡æºç›£æ§åœ¨é€™è¡Œåˆ·æ–°

    # å•Ÿå‹•å¯¦æ™‚ç›£æ§åŸ·è¡Œç·’
    monitor_thread = threading.Thread(target=monitor_resources, kwargs={'interval':1}, daemon=True)
    monitor_thread.start()

    try:
        # è¨­å®šæ¸¬è©¦åœ–ç‰‡è·¯å¾‘
        image_folder = Path("test_images")
        image_paths = sorted(image_folder.glob("*.png")) + sorted(image_folder.glob("*.jpg"))

        if len(image_paths) == 0:
            raise FileNotFoundError("âš ï¸ æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡ï¼Œè«‹æ”¾åœ–ç‰‡åˆ° test_images è³‡æ–™å¤¾ä¸‹ï¼")

        # è™•ç†åœ–ç‰‡ï¼ˆé€²åº¦æ¢åœ¨å–®ç¨ä¸€è¡Œï¼‰
        images_list = []
        for p in tqdm(image_paths, desc="è™•ç†åœ–ç‰‡", position=0):
            images_list.append(process_single_image(p))
        images = torch.stack(images_list)

        # è¨­å®šè£ç½®
        device = "cpu"
        dtype = torch.float32

        # è¼‰å…¥æ¨¡å‹
        model, loading_info = load_model_from_huggingface("egeozsoy/EndoViT")
        model = model.to(device, dtype)
        print("\nâœ… æ¨¡å‹è¼‰å…¥å®Œæˆ")

        # ç§»å‹•åœ–ç‰‡åˆ°è£ç½®
        images = images.to(device, dtype)

        # å‰å‘å‚³æ’­ï¼ˆé€²åº¦æ¢åœ¨å–®ç¨ä¸€è¡Œï¼‰
        features_list = []
        for i in tqdm(range(images.shape[0]), desc="è¨ˆç®—ç‰¹å¾µ", position=0):
            with torch.no_grad():
                feat = model.forward_features(images[i:i+1])
                features_list.append(feat)
        features = torch.cat(features_list, dim=0)

        # å­˜æª”
        torch.save(features.cpu(), "features.pt")
        print("ğŸ’¾ ç‰¹å¾µå·²å­˜æˆ features.pt")

    except KeyboardInterrupt:
        print("\nâš ï¸ åµæ¸¬åˆ° Ctrl+Cï¼Œä¸­æ–·ç¨‹å¼...")

    finally:
        # åœæ­¢ç›£æ§
        monitor_resources.running = False
        monitor_thread.join()

    end_time = time.time()
    print(f"â±ï¸ ç¸½åŸ·è¡Œæ™‚é–“: {end_time - start_time:.1f} ç§’")
